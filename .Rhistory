if (!is.null(colnames(X))) colnames(centroids) <- colnames(X) else colnames(centroids) <- paste0("Metric", 1:ncol(X))
return(list(assignment=assgn, centroids=centroids)) # return list
}
####################################################################################
### Example: cityweather ####
# compare K_means function to built-in Kmeans function
# load data
load(paste0(map_data, "cityweather.RData"))
# run built-in function
set.seed(42)
builtin <- kmeans(cityweather, 4)
# get initial assignment from built-in K-means function
## note that K_means can find local minima as solutions (see HTF)
## --> use initial assignment of built-in function for comparison
set.seed(42)
initial <- kmeans(cityweather, 4, iter.max=1)$cluster
# run own clustering function using initial assignment
own <- K_means(cityweather, 4, initial)
# compare
table(own$assignment, builtin$cluster)
####################################################################################
### Implementation of a simple k-means cluster algorithm  ####
# see: Algorithm 14.1 in Hastie, Tibshirani & Friedman (2017): Elements of Statistical Learning
####################################################################################
#### Settings ####
# set path to data set
map_data <- paste0(getwd(), "/Data/")
####################################################################################
#### K-means function ####
K_means <- function(mat, K, initial = NA){
# mat: numeric data matrix
# K: desired number of clusters
# initial: (optional) initial allocation matrix
# checks, transformations
if (!is.matrix(mat)) X <- as.matrix(mat) else X <- mat
if (!is.numeric(X)) warnings("Please specifiy a numeric data matrix.")
n <- nrow(X)
# initialize empty matrices
centroids <- matrix(NA, nrow=K, ncol=ncol(X))
dists <- matrix(NA, nrow=n, ncol=K)
# set initial assignment
old_assgn <- rep(0,n) # no assignment
if (any(is.na(initial))) assgn <- sample(1:K,n,replace=T) else assgn <- initial
while (any(assgn != old_assgn)){
old_assgn <- assgn
for (k in 1:K){
# calculate cluster centroid
centroids[k,] <- apply(X[which(assgn==k), , drop=FALSE], 2, mean)
# calculate euclidean distance
dists[,k] <- apply(X, 1, function(x) {
dist(rbind(centroids[k,], x), method="euclidean", p=2)
})
}
# assign to closest cluster
assgn <- apply(dists, 1, which.min)
}
# assign names for output
if (!is.null(rownames(X))) names(assgn) <- rownames(X) else names(assgn) <- paste0("Row", 1:nrow(X))
rownames(centroids) <- paste(1:K) # each row is a cluster
if (!is.null(colnames(X))) colnames(centroids) <- colnames(X) else colnames(centroids) <- paste0("Metric", 1:ncol(X))
return(list(assignment=assgn, centroids=centroids)) # return list
}
####################################################################################
### Example: cityweather ####
# compare K_means function to built-in Kmeans function
# load data
load(paste0(map_data, "cityweather.RData"))
# run built-in function
set.seed(42)
builtin <- kmeans(cityweather, 4)
# get initial assignment from built-in K-means function
## note that K_means can find local minima as solutions (see HTF)
## --> use initial assignment of built-in function for comparison
set.seed(42)
initial <- kmeans(cityweather, 4, iter.max=1)$cluster
# run own clustering function using initial assignment
own <- K_means(cityweather, 4, initial)
# compare
table(own$assignment, builtin$cluster)
K_means <- function(mat, K, w, initial = NA){
# mat: numeric data matrix
# K: desired number of clusters
# initial: (optional) initial allocation matrix
# checks, transformations
if (!is.matrix(mat)) X <- as.matrix(mat) else X <- mat
if (!is.numeric(X)) warnings("Please specifiy a numeric data matrix.")
n <- nrow(X)
# initialize empty matrices
centroids <- matrix(NA, nrow=K, ncol=ncol(X))
dists <- matrix(NA, nrow=n, ncol=K)
# set initial assignment
old_assgn <- rep(0,n) # no assignment
if (any(is.na(initial))) assgn <- sample(1:K,n,replace=T) else assgn <- initial
while (any(assgn != old_assgn)){
old_assgn <- assgn
for (k in 1:K){
# calculate cluster centroid
if (!missing(w)){
} else{
centroids[k,] <- apply(X[which(assgn==k), , drop=FALSE], 2, mean)
}
# calculate euclidean distance
dists[,k] <- apply(X, 1, function(x) {
dist(rbind(centroids[k,], x), method="euclidean", p=2)
})
}
# assign to closest cluster
assgn <- apply(dists, 1, which.min)
}
# assign names for output
if (!is.null(rownames(X))) names(assgn) <- rownames(X) else names(assgn) <- paste0("Row", 1:nrow(X))
rownames(centroids) <- paste(1:K) # each row is a cluster
if (!is.null(colnames(X))) colnames(centroids) <- colnames(X) else colnames(centroids) <- paste0("Metric", 1:ncol(X))
return(list(assignment=assgn, centroids=centroids)) # return list
}
# add weights
weights <- 1:nrow(cityweather)
# run built-in function
set.seed(42)
builtin <- kmeans(cityweather, 4)
# get initial assignment from built-in K-means function
## note that K_means can find local minima as solutions (see HTF)
## --> use initial assignment of built-in function for comparison
set.seed(42)
initial <- kmeans(cityweather, 4, iter.max=1)$cluster
# run own clustering function using initial assignment
own <- K_means(cityweather, 4, initial)
own <- K_means(cityweather, K=4, initial = initial)
table(own$assignment, builtin$cluster)
# add weights
weights <- 1:nrow(cityweather)
# run built-in function
set.seed(42)
builtin <- kmeans(cityweather, 4)
# get initial assignment from built-in K-means function
## note that K_means can find local minima as solutions (see HTF)
## --> use initial assignment of built-in function for comparison
set.seed(42)
initial <- kmeans(cityweather, 4, iter.max=1)$cluster
# run own clustering function using initial assignment
own <- K_means(cityweather, K=4, initial = initial)
# compare
table(own$assignment, builtin$cluster)
K_means <- function(mat, K, w, initial = NA){
# mat: numeric data matrix
# K: desired number of clusters
# initial: (optional) initial allocation matrix
# checks, transformations
if (!is.matrix(mat)) X <- as.matrix(mat) else X <- mat
if (!is.numeric(X)) warnings("Please specifiy a numeric data matrix.")
n <- nrow(X)
# initialize empty matrices
centroids <- matrix(NA, nrow=K, ncol=ncol(X))
dists <- matrix(NA, nrow=n, ncol=K)
# set initial assignment
old_assgn <- rep(0,n) # no assignment
if (any(is.na(initial))) assgn <- sample(1:K,n,replace=T) else assgn <- initial
while (any(assgn != old_assgn)){
old_assgn <- assgn
for (k in 1:K){
# calculate cluster centroid
if (!missing(w)){
centroids[k,] <- apply(X[which(assgn==k), , drop=FALSE], 2, function(x) sum(x * w[assgn==k])/sum(w[assgn==k]))
} else{
centroids[k,] <- apply(X[which(assgn==k), , drop=FALSE], 2, mean)
}
# calculate euclidean distance
dists[,k] <- apply(X, 1, function(x) {
dist(rbind(centroids[k,], x), method="euclidean", p=2)
})
}
# assign to closest cluster
assgn <- apply(dists, 1, which.min)
}
# assign names for output
if (!is.null(rownames(X))) names(assgn) <- rownames(X) else names(assgn) <- paste0("Row", 1:nrow(X))
rownames(centroids) <- paste(1:K) # each row is a cluster
if (!is.null(colnames(X))) colnames(centroids) <- colnames(X) else colnames(centroids) <- paste0("Metric", 1:ncol(X))
return(list(assignment=assgn, centroids=centroids)) # return list
}
# load data
load(paste0(map_data, "cityweather.RData"))
# add weights
weights <- 1:nrow(cityweather)
# run built-in function
set.seed(42)
builtin <- kmeans(cityweather, 4)
# get initial assignment from built-in K-means function
## note that K_means can find local minima as solutions (see HTF)
## --> use initial assignment of built-in function for comparison
set.seed(42)
initial <- kmeans(cityweather, 4, iter.max=1)$cluster
# run own clustering function using initial assignment
own <- K_means(cityweather, K=4, initial = initial)
# compare
table(own$assignment, builtin$cluster)
own <- K_means(cityweather, K=4, w=weights, initial = initial)
table(own$assignment, builtin$cluster)
# load data
load(paste0(map_data, "cityweather.RData"))
# add weights
weights <- 1:nrow(cityweather)
# run built-in function
set.seed(42)
builtin <- kmeans(cityweather, 4)
# get initial assignment from built-in K-means function
## note that K_means can find local minima as solutions (see HTF)
## --> use initial assignment of built-in function for comparison
set.seed(42)
initial <- kmeans(cityweather, 4, iter.max=1)$cluster
# run own clustering function using initial assignment
own <- K_means(cityweather, K=4, w=weights, initial = initial)
# compare
table(own$assignment, builtin$cluster)
# calculate mean
m <- mean(c(4,5,2,3,1,4,3,6,3,2,4,1))
# calculate standard deviation
s <- sd(c(4,5,2,3,1,4,3,6,3,2,4,1))
# calculate n
n <- length(c(4,5,2,3,1,4,3,6,3,2,4,1))
n
sd
s
s/sqrt(n)
1,96*s/sqrt(n)
1.96*s/sqrt(n)
# ====== simulate some data ==========
model = m2.mini.new(10,serial = F)
NNs   = array(300000/model$nf,model$nf)
NNm   = array(30000/model$nf^2,c(model$nf,model$nf))
sdata = m2.mini.simulate.stayers(model,NNs)
jdata = m2.mini.simulate.movers(model,NNm)
# randomly assign firm IDs
sdata[,f1:=paste("F",j1 + model$nf*(sample.int(.N/50,.N,replace=T)-1),sep=""),j1]
sdata[,j1b:=j1]
jdata[,j1c:=j1]
jdata[,f1:=sample( unique(sdata[j1b==j1c,f1])    ,.N,replace=T),j1c]
jdata[,f2:=sample( unique(sdata[j1b==j2,f1])    ,.N,replace=T),j2]
# combine the movers and stayers, ad stands for all data:
ad = list(sdata=sdata,jdata=jdata)
ad
x
ad$sdata
ad$sdata$y1
ecdf(ad$sdata$y1)
quantile(ecdf(ad$sdata$y1))
quantile(ad$sdata$y1)
## The projection keeps the vertex attributes
M <- matrix(0, nrow=5, ncol=3)
rownames(M) <- c("Alice", "Bob", "Cecil", "Dan", "Ethel")
colnames(M) <- c("Party", "Skiing", "Badminton")
M[] <- sample(0:1, length(M), replace=TRUE)
M
g2 <- graph_from_incidence_matrix(M)
install.packages("igraph")
library(igraph)
## The projection keeps the vertex attributes
M <- matrix(0, nrow=5, ncol=3)
rownames(M) <- c("Alice", "Bob", "Cecil", "Dan", "Ethel")
colnames(M) <- c("Party", "Skiing", "Badminton")
M[] <- sample(0:1, length(M), replace=TRUE)
M
g2 <- graph_from_incidence_matrix(M)
g2$name <- "Event network"
proj2 <- bipartite_projection(g2)
print(proj2[[1]], g=TRUE, e=TRUE)
print(proj2[[2]], g=TRUE, e=TRUE)
g2
g2
M
## The projection keeps the vertex attributes
M <- matrix(0, nrow=5, ncol=3)
rownames(M) <- c("Alice", "Bob", "Cecil", "Dan", "Ethel")
colnames(M) <- c("Party", "Skiing", "Badminton")
M[] <- sample(0:1, length(M), replace=TRUE)
M
g2 <- graph_from_incidence_matrix(M)
g2
is.bipartite(g2)
V(g2)$type
V(g2)
bipartite_projection(g2, multiplicity = TRUE, which = "true")
bipartite_projection(g2, multiplicity = TRUE, which = "FALSE")
E(g2)
E(proj2([[1]]))
E(proj2[[1]])()
E(proj2[[1]]))
E(proj2[[1]])
E(proj2[[1]])$weight
M <- matrix(0, nrow=5, ncol=3)
rownames(M) <- c("Alice", "Bob", "Cecil", "Dan", "Ethel")
colnames(M) <- c("Party", "Skiing", "Badminton")
M[] <- sample(0:1, length(M), replace=TRUE)
M
E(proj2[[2]])$weight
E(proj2[[2]])
g2 <- graph_from_incidence_matrix(M)
g2$name <- "Event network"
proj2 <- bipartite_projection(g2)
print(proj2[[1]], g=TRUE, e=TRUE)
print(proj2[[2]], g=TRUE, e=TRUE)
E(proj2[[2]])$weight
E(proj2[[2]])
M
proj2 <- bipartite_projection(g2, multiplicity = TRUE, which="true", remove.type = TRUE)
proj2
E(proj2)
E(proj2)$weights
E(proj2)$weight
embed_laplacian_matrix(proj2, no = 2, which = "sa", type = "I-DAD", scaled=TRUE)
#' Weighted K-Means Clustering with Weights on Observations
#'
#' Perform K-Means algorithm on observations with given weights.
#' @param x An \emph{n} by \emph{p} numeric data matrix, and \emph{n} is the number of observations and \emph{p} the number of features.
#' @param K The number of clusters. Omitted if \code{centers} are provided.
#' @param weight A vector of \emph{n} positive elements representing weights on observations.
#' @param centers A \emph{K} by \emph{p} matrix indicating initial (distinct) cluster centers.
#' @param nstart The number of initial random sets chosen from (distinct) rows in \code{x}. Omitted if \code{centers} is provided. Default is 20.
#' @param algorithm Character; either "\code{Hartigan-Wong}" or "\code{Forgy}". Default is "\code{Hartigan-Wong}".
#' @keywords Weighted K-Means Clustering
#' @return The function returns a list of the following components:
#' \item{centers}{the centers of the clustering result.}
#' \item{cluster}{a vector of integers (from \code{1:k}) indicating the cluster to which each observation is allocated.}
#' \item{weight}{a vector of non-zero weights in the input vector \code{weight}.}
#' \item{wcss}{normalized within-cluster sum of squares, i.e. the objective divided by \code{sum(weight)}.}
#' @family sparse weighted K-Means functions
#' @author Wenyu Zhang
#' @importFrom stats na.omit rnorm sd
#' @importFrom graphics lines arrows plot
#' @importFrom Rcpp evalCpp
#' @useDynLib SWKM
#' @export
#' @examples
#' \dontrun{
#' set.seed(1)
#' data("DMdata")
#' # data preprocessing
#' data <- t(DMdata$data)
#' data_rank <- apply(data, 2, rank)
#' data_rank_center<- t(t(data_rank) - colMeans(data_rank))
#' data_rank_center_scale <- t(t(data_rank_center)/apply(data_rank_center, 2, sd))
#' data_processed <-  t(data_rank_center_scale)
#' # tune the number of cluster K
#' # nperms and nstart are set to be small in order to save computation time
#' cK <- ChooseK(data_processed[-DMdata$noisy.label,],nClusters = 1:6,nperms = 10,nstart = 5)
#' plot(cK)
#' K <- cK$OptimalK
#' # tune weight
#'   res.tuneU <- kmeans.weight.tune(x = data_processed,K = K,
#'   noisy.lab = DMdata$noisy.label,nperms = 10,nstart = 5)
#' plot(res.tuneU)
#' # perform weighted K-means
#' res <- kmeans.weight(x = data_processed,K = K,weight = res.tuneU$bestweight)
#' # check the result
#' table(res$cluster,DMdata$true.label)
#' }
kmeans.weight <- function(x,K=NULL,weight=NULL,centers=NULL,nstart=20,algorithm="Hartigan-Wong"){
if (is.null(K) && is.null(centers))
stop("Must provide either K or centers.")
if (!is.null(K) && !is.null(centers) && nrow(centers) != K )
stop("If K and centers both are provided, then nrow(centers) must equal K!!!")
if (!is.null(centers) && ncol(centers)!=ncol(x))
stop("If centers is provided, then ncol(centers) must equal ncol(x).")
if(!is.null(weight) && length(weight)!=nrow(x))
stop("length(weight) must equal nrow(x).")
if(is.null(weight))    weight <- rep(1,nrow(x))
if(any(weight<0)) stop("weight should be larger than or equal to 0.")
#### if there exists some zero weight, the corresponding data will be omitted. ####
if(any(weight==0)) {
x <- x[weight!=0,]
weight <- weight[weight!=0]
}
if(is.null(centers))
centers <- x[sample(nrow(x),K),]
else {
nstart <- 1
K<- nrow(centers)
}
if(length(weight)<=K)
stop("number of positive weights should be larger than K and nrow(centers)!")
if(K==1){
centers <- weight/sum(weight)%*%x
wcss <- sum(weight/sum(weight)%*%(sweep(x,2,centers)^2))
cluster <- rep(1,nrow(x))
return(list(centers=centers,cluster=cluster,weight=weight,wcss=wcss))
}
WCSS.min <- Inf
for(ns in 1:nstart){
if(ns > 1) centers <- x[sample(nrow(x),K),]
niter <- 0
maxiter <- 10
#### Forgy algorithm ####
if(algorithm=="Forgy"){
centers.old <- centers+1
while(niter<maxiter && sum(abs(centers-centers.old))>1e-4){
distmat2 <- apply(centers, 1, function(y){colSums((t(x) - y)^2)})
Cs <- apply(distmat2, 1, which.min)
centers.old <- centers
if(length(unique(Cs))!=K)
stop("empty cluster: try a better set of initial centers.")
for(k in unique(Cs))
centers[k,] <- (weight[Cs==k]/sum(weight[Cs==k]))%*%x[Cs==k,,drop=FALSE]
niter <- niter + 1
}
WCSS <- GetWCSS.weight(x,Cs,weight/sum(weight))$wcss
if(WCSS<WCSS.min) {
WCSS.min <- WCSS
centers.min <- centers
Cs.min <- Cs
}
}
#### Hartigan Algorithm ####
if(algorithm=="Hartigan-Wong"){
out <- kmeans_hartigan(x,centers,weight/sum(weight))
WCSS <- out$wcss
if(WCSS<WCSS.min) {
WCSS.min <- WCSS
centers.min <- out$centers
Cs.min <- out$cluster
}
}
}
return(list(centers=centers.min,cluster=Cs.min,weight=weight,wcss=WCSS.min))
}
GetWCSS.weight <- function(x, Cs, weight, ws = NULL){
wcss.perfeature <- numeric(ncol(x))
for (k in unique(Cs)) {
whichers <- (Cs == k)
if (sum(whichers) > 1) {
centers <- (weight[whichers]/sum(weight[whichers]))%*%x[whichers,]
wcss.perfeature <- wcss.perfeature + weight[whichers]%*%(sweep(x[whichers,],2,centers)^2)
}
}
centers <- (weight/sum(weight))%*%x
bcss.perfeature <- weight%*%(sweep(x,2,centers)^2) - wcss.perfeature
if (!is.null(ws))
return(list(wcss.perfeature = wcss.perfeature, wcss.ws = sum(wcss.perfeature * ws),
bcss.perfeature = bcss.perfeature, bcss.ws = sum(bcss.perfeature * ws)))
if (is.null(ws))
return(list(wcss.perfeature = wcss.perfeature, wcss = sum(wcss.perfeature),
bcss.perfeature = bcss.perfeature, bcss = sum(bcss.perfeature)))
}
M
K_mean(M, 10)
K_means(M, 10)
kmeans.weight()
kmeans.weight(m)
kmeans.weight(m, 10)
kmeans.weight(m, 1)
kmeans.weight(m, 2)
kmeans.weight(m, K=2)
nrow(m)
m
kmeans.weight(m, 10)
kmeans.weight(M, 10)
kmeans.weight(M, 2)
kmeans.weight(M, 1)
library(devtools)
devtools::install_github("Van1yu3/SWKM")
library(devtools)
devtools::install_github("Van1yu3/SWKM")
RcppArmadillo
install.packages("RcppArmadillo")
library(devtools)
devtools::install_github("Van1yu3/SWKM")
? install_github
library(data.table)
k <- 1
tt <- data.table(M)
tt
tt[, .(p`k` = sum(Badminton))]
tt[, .("a" = sum(Badminton))]
tt[, .(paste0("tt", k) = sum(Badminton))]
tt[, .(paste0("tt") = sum(Badminton))]
tt[, .("a" = sum(Badminton))]
tt[, .(k = sum(Badminton))]
tt[, .(get(k) = sum(Badminton))]
tt[, .(eval(as.symbol(k)) = sum(Badminton))]
tt[, .(1 = sum(Badminton))]
tt[, .(a = sum(Badminton))]
tt[, .(paste0("a", k) = sum(Badminton))]
tt[, .("a1" = sum(Badminton))]
tt[, .(get(paste0("a", k)) = sum(Badminton))]
tt[, .(eval(paste0("a", k)) = sum(Badminton))]
tt[, .(as.symbol(paste0("a", k)) = sum(Badminton))]
tt[, .(eval(as.symbol(paste0("a", k))) = sum(Badminton))]
tt[, .(eval(get(paste0("a", k))) = sum(Badminton))]
tt[, .(paste0("a", k) = sum(Badminton))]
tt[, .(`paste0("a", k)` = sum(Badminton))]
tt[, .(paste0("a", k) = sum(Badminton))]
for (i in 1:10) warning("tt")
? kmeans
kmeans(m)
kmeans(M)
kmeans(M, centers=2)
kmeans(M, centers=2, nstart=10)
kmeans(M, centers=2, nstart=100)
kmeans(M, centers=2, nstart=1000)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=1)
kmeans(M, centers=2, nstart=2)
kmeans(M, centers=2, nstart=50)
kmeans(M, centers=2, nstart=20)
kmeans(M, centers=2, nstart=30)
kmeans(M, centers=2, nstart=40)
hclust()
? hclust
